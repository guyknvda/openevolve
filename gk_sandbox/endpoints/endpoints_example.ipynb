{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d86f85d",
   "metadata": {},
   "source": [
    "# Endpoint Access\n",
    "The goal of this notebook is to examine the ability of a kernel agent to generate kernels that are memory bw limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd273f",
   "metadata": {},
   "source": [
    "## Setup and Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22527694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "root_dir = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
    "root_dir\n",
    "sys.path.append(root_dir)\n",
    "from gk_sandbox.endpoints.endpoints import MODEL_NAME_TO_ID\n",
    "env_path=os.path.join(root_dir,'gk_sandbox','endpoints','.env')\n",
    "env_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf25e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display_markdown\n",
    "# Remember to load the environment variables. You should have the Groq API Key in there :)\n",
    "load_dotenv(env_path)\n",
    "api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpt=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "perlab_api_key = os.getenv(\"PERFLAB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3dc761",
   "metadata": {},
   "source": [
    "### Quick sanity check and usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657fecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=MODEL_NAME_TO_ID['claude']\n",
    "client = AzureOpenAI(azure_endpoint=azure_endpt,\n",
    "                     api_version=api_version,\n",
    "                     api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77787fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23304751",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergesort_code = client.chat.completions.create(\n",
    "    messages=generation_chat_history,\n",
    "    # model=\"llama3-70b-8192\"\n",
    "    model=model_id\n",
    ").choices[0].message.content\n",
    "\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": mergesort_code\n",
    "    }\n",
    ")\n",
    "display_markdown(mergesort_code, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e683f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "\n",
    "    # Divide the array into two halves\n",
    "    mid = len(arr) // 2\n",
    "    left = arr[:mid]\n",
    "    right = arr[mid:]\n",
    "\n",
    "    # Recursively sort both halves\n",
    "    left = merge_sort(left)\n",
    "    right = merge_sort(right)\n",
    "\n",
    "    # Merge the sorted halves\n",
    "    return merge(left, right)\n",
    "\n",
    "def merge(left, right):\n",
    "    result = []\n",
    "    i, j = 0, 0\n",
    "\n",
    "    # Compare elements from both lists and add the smaller one to the result\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] <= right[j]:\n",
    "            result.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(right[j])\n",
    "            j += 1\n",
    "\n",
    "    # Add any remaining elements from the left list\n",
    "    while i < len(left):\n",
    "        result.append(left[i])\n",
    "        i += 1\n",
    "\n",
    "    # Add any remaining elements from the right list\n",
    "    while j < len(right):\n",
    "        result.append(right[j])\n",
    "        j += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "# Test the merge_sort function\n",
    "arr = [64, 34, 25, 12, 22, 11, 90]\n",
    "print(\"Original array:\", arr)\n",
    "sorted_arr = merge_sort(arr)\n",
    "print(\"Sorted array:\", sorted_arr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080eb0a0",
   "metadata": {},
   "source": [
    "# Accessing local (HF) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-32B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-32B\",torch_dtype=torch.bfloat16,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "generation_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way to generate the prompt\n",
    "prompt = \"\"\n",
    "for message in generation_chat_history:\n",
    "    if message[\"role\"] == \"system\":\n",
    "        prompt += f\"System: {message['content']}\\n\"\n",
    "    elif message[\"role\"] == \"user\":\n",
    "        prompt += f\"User: {message['content']}\\n\"\n",
    "    elif message[\"role\"] == \"assistant\":\n",
    "        prompt += f\"Assistant: {message['content']}\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "prompt_length = inputs[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2bc33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more direct\n",
    "inputs2 = tokenizer.apply_chat_template(\n",
    "    generation_chat_history,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "prompt_length = inputs2.shape[1]  # Number of tokens in the prompt\n",
    "# inputs = {k: v.to(model.device) for k, v in inputs2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd5b1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs2,\n",
    "        max_new_tokens=4096,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc57601",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = output[0][prompt_length:]\n",
    "\n",
    "# 5. Decode only the new tokens\n",
    "assistant_response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b9480",
   "metadata": {},
   "source": [
    "## Testing the generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bb35e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    \"\"\"\n",
    "    Sorts a list using the Merge Sort algorithm.\n",
    "    \n",
    "    Args:\n",
    "        arr (list): The list to be sorted.\n",
    "        \n",
    "    Returns:\n",
    "        list: A new sorted list.\n",
    "    \"\"\"\n",
    "    # Base case: if the array has one element or is empty, it's already sorted\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    \n",
    "    # Divide the array into two halves\n",
    "    mid = len(arr) // 2\n",
    "    left_half = merge_sort(arr[:mid])  # Recursively sort the left half\n",
    "    right_half = merge_sort(arr[mid:])  # Recursively sort the right half\n",
    "    \n",
    "    # Combine the sorted halves\n",
    "    return merge(left_half, right_half)\n",
    "\n",
    "\n",
    "def merge(left, right):\n",
    "    \"\"\"\n",
    "    Merges two sorted lists into a single sorted list.\n",
    "    \n",
    "    Args:\n",
    "        left (list): The first sorted list.\n",
    "        right (list): The second sorted list.\n",
    "        \n",
    "    Returns:\n",
    "        list: A merged sorted list.\n",
    "    \"\"\"\n",
    "    merged = []  # Result list\n",
    "    i = j = 0    # Pointers for left and right lists\n",
    "    \n",
    "    # Merge the two lists by comparing elements\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] <= right[j]:  # Ensure stability by using <=\n",
    "            merged.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            j += 1\n",
    "    \n",
    "    # Add any remaining elements from left and right\n",
    "    merged.extend(left[i:])\n",
    "    merged.extend(right[j:])\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsorted = [34, 7, 23, 32, 5, 62]\n",
    "unsorted = [64, 34, 25, 12, 22, 11, 90]\n",
    "sorted_list = merge_sort(unsorted)\n",
    "print(sorted_list)  # Output: [5, 7, 23, 32, 34, 62]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e979c5",
   "metadata": {},
   "source": [
    "# Accessing inference service\n",
    "using ollama :\n",
    "- setup docker network : \n",
    "```bash\n",
    "docker network create llmnet\n",
    "```\n",
    "- launch an ollama container \n",
    "```bash\n",
    "docker run -d  --gpus all --name ollama   --network llmnet   -p 11434:11434  ollama/ollama\n",
    "```\n",
    "\n",
    "Note: make sure that this notebook's container is also launched with `--network llmnet`\n",
    "\n",
    "- attach to the ollama container to pull the model\n",
    "```bash\n",
    "docker exec -it ollama bash\n",
    "```\n",
    "- from within the container, pull the model\n",
    "```\n",
    "ollama pull qwen3:8b\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b23a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d7461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(messages):\n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            prompt += f\"System: {msg['content']}\\n\"\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            prompt += f\"User: {msg['content']}\\n\"\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {msg['content']}\\n\"\n",
    "    response = requests.post(\n",
    "        \"http://ollama:11434/api/generate\",\n",
    "        json={\"model\": \"qwen3:8b\", \"prompt\": prompt}\n",
    "    )\n",
    "    print(response.status_code)\n",
    "    full_text = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode('utf-8'))\n",
    "            # The generated text is usually in the 'response' field\n",
    "            full_text += data.get(\"response\", \"\")\n",
    "    return full_text\n",
    "\n",
    "def get_response_openai(messages, model=\"qwen3:8b\", base_url=\"http://ollama:11434/v1\"):\n",
    "    # Create a client that points to the Ollama OpenAI-compatible endpoint\n",
    "    client = openai.OpenAI(\n",
    "        api_key=\"ollama\",  # Any string, Ollama doesn't check it\n",
    "        base_url=base_url\n",
    "    )\n",
    "    # Call the chat completion endpoint\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    # Extract the assistant's reply\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aceead2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"You are a Python programmer tasked with generating high quality Python code.Your task is to Generate the best content possible for the user's request. If the user provides critique,respond with a revised version of your previous attempt.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Generate a Python implementation of the Merge Sort algorithm /nothink'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "generation_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Certainly! Below is a high-quality Python implementation of the **Merge Sort** algorithm. This version is clean, well-commented, and follows best practices for readability and efficiency.\n",
      "\n",
      "```python\n",
      "def merge_sort(arr):\n",
      "    \"\"\"\n",
      "    Sorts a list in ascending order using the Merge Sort algorithm.\n",
      "    \n",
      "    Parameters:\n",
      "    arr (list): The list to be sorted.\n",
      "    \n",
      "    Returns:\n",
      "    list: The sorted list.\n",
      "    \"\"\"\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "\n",
      "    # Split the array into two halves\n",
      "    mid = len(arr) // 2\n",
      "    left_half = merge_sort(arr[:mid])\n",
      "    right_half = merge_sort(arr[mid:])\n",
      "\n",
      "    # Merge the sorted halves\n",
      "    return merge(left_half, right_half)\n",
      "\n",
      "\n",
      "def merge(left, right):\n",
      "    \"\"\"\n",
      "    Merges two sorted lists into one sorted list.\n",
      "    \n",
      "    Parameters:\n",
      "    left (list): The first sorted list.\n",
      "    right (list): The second sorted list.\n",
      "    \n",
      "    Returns:\n",
      "    list: The merged sorted list.\n",
      "    \"\"\"\n",
      "    merged = []\n",
      "    i = j = 0\n",
      "\n",
      "    while i < len(left) and j < len(right):\n",
      "        if left[i] < right[j]:\n",
      "            merged.append(left[i])\n",
      "            i += 1\n",
      "        else:\n",
      "            merged.append(right[j])\n",
      "            j += 1\n",
      "\n",
      "    # Add any remaining elements from left or right\n",
      "    merged.extend(left[i:])\n",
      "    merged.extend(right[j:])\n",
      "\n",
      "    return merged\n",
      "```\n",
      "\n",
      "### ✅ Features:\n",
      "- **Recursive Implementation**: The algorithm is implemented recursively, which is the standard approach for Merge Sort.\n",
      "- **Efficient Time Complexity**: O(n log n) in all cases (best, average, and worst).\n",
      "- **Space Complexity**: O(n) due to the additional space used for merging.\n",
      "- **Well-Documented**: Each function includes a docstring explaining its purpose, parameters, and return value.\n",
      "\n",
      "### 📌 Example Usage:\n",
      "```python\n",
      "if __name__ == \"__main__\":\n",
      "    unsorted_list = [38, 27, 43, 3, 9, 82, 10]\n",
      "    sorted_list = merge_sort(unsorted_list)\n",
      "    print(\"Sorted list:\", sorted_list)\n",
      "```\n",
      "\n",
      "### 📌 Output:\n",
      "```\n",
      "Sorted list: [3, 9, 10, 27, 38, 43, 82]\n",
      "```\n",
      "\n",
      "Let me know if you'd like a version that sorts in-place or handles other data types!\n"
     ]
    }
   ],
   "source": [
    "# using native ollama api\n",
    "response = get_response(generation_chat_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d352552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Certainly! Below is a high-quality Python implementation of the **Merge Sort** algorithm. This implementation is clean, efficient, and includes proper comments for clarity.\n",
      "\n",
      "```python\n",
      "def merge_sort(arr):\n",
      "    \"\"\"\n",
      "    Sorts a list in ascending order using the Merge Sort algorithm.\n",
      "    \n",
      "    Parameters:\n",
      "    arr (list): The list to be sorted.\n",
      "    \n",
      "    Returns:\n",
      "    list: The sorted list.\n",
      "    \"\"\"\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    \n",
      "    # Divide the array into two halves\n",
      "    mid = len(arr) // 2\n",
      "    left_half = merge_sort(arr[:mid])\n",
      "    right_half = merge_sort(arr[mid:])\n",
      "    \n",
      "    # Merge the sorted halves\n",
      "    return merge(left_half, right_half)\n",
      "\n",
      "def merge(left, right):\n",
      "    \"\"\"\n",
      "    Merges two sorted lists into one sorted list.\n",
      "    \n",
      "    Parameters:\n",
      "    left (list): The first sorted list.\n",
      "    right (list): The second sorted list.\n",
      "    \n",
      "    Returns:\n",
      "    list: The merged sorted list.\n",
      "    \"\"\"\n",
      "    merged = []\n",
      "    i = j = 0\n",
      "    \n",
      "    while i < len(left) and j < len(right):\n",
      "        if left[i] < right[j]:\n",
      "            merged.append(left[i])\n",
      "            i += 1\n",
      "        else:\n",
      "            merged.append(right[j])\n",
      "            j += 1\n",
      "    \n",
      "    # Add any remaining elements from the left or right list\n",
      "    merged.extend(left[i:])\n",
      "    merged.extend(right[j:])\n",
      "    \n",
      "    return merged\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    unsorted_list = [38, 27, 43, 3, 9, 82, 10]\n",
      "    sorted_list = merge_sort(unsorted_list)\n",
      "    print(\"Sorted list:\", sorted_list)\n",
      "```\n",
      "\n",
      "### Key Features:\n",
      "- **Recursive Divide-and-Conquer**: The `merge_sort` function divides the array into halves recursively.\n",
      "- **Efficient Merging**: The `merge` function merges two sorted arrays in linear time, resulting in an overall time complexity of **O(n log n)**.\n",
      "- **In-Place Sorting (Not Actually In-Place)**: Although merge sort is not an in-place algorithm, the implementation is clean and easy to understand.\n",
      "\n",
      "### Example Output:\n",
      "```\n",
      "Sorted list: [3, 9, 10, 27, 38, 43, 82]\n",
      "```\n",
      "\n",
      "Let me know if you'd like a version with in-place sorting or additional functionality.\n"
     ]
    }
   ],
   "source": [
    "# using openai api\n",
    "response = get_response_openai(generation_chat_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f6e48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    \"\"\"\n",
    "    Sorts an array using the Merge Sort algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    arr (list): The list of elements to be sorted.\n",
    "    \n",
    "    Returns:\n",
    "    list: A new list containing all elements from the original list, sorted in ascending order.\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr  # Base case: single-element list is already sorted\n",
    "    \n",
    "    # Split the array into left and right halves\n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort(arr[:mid])  # Recursively sort the left half\n",
    "    right = merge_sort(arr[mid:])  # Recursively sort the right half\n",
    "    \n",
    "    # Merge the sorted halves\n",
    "    return merge(left, right)\n",
    "\n",
    "def merge(left, right):\n",
    "    \"\"\"\n",
    "    Merges two sorted lists into a single sorted list.\n",
    "    \n",
    "    Parameters:\n",
    "    left (list): The first sorted list.\n",
    "    right (list): The second sorted list.\n",
    "    \n",
    "    Returns:\n",
    "    list: A new list containing all elements from both input lists, sorted in ascending order.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = j = 0\n",
    "    \n",
    "    # Merge elements from both lists\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] < right[j]:\n",
    "            merged.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            j += 1\n",
    "    \n",
    "    # Add any remaining elements from the left or right list\n",
    "    merged.extend(left[i:])\n",
    "    merged.extend(right[j:])\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsorted = [34, 7, 23, 32, 5, 62]\n",
    "unsorted = [64, 34, 25, 12, 22, 11, 90]\n",
    "sorted_list = merge_sort(unsorted)\n",
    "print(sorted_list)  # Output: [5, 7, 23, 32, 34, 62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46fbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
