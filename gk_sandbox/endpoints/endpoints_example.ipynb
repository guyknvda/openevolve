{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d86f85d",
   "metadata": {},
   "source": [
    "# Endpoint Access\n",
    "The goal of this notebook is to examine the ability of a kernel agent to generate kernels that are memory bw limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd273f",
   "metadata": {},
   "source": [
    "## Setup and Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22527694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/gkoren/code/local/kgen_problems/endpoints/.env'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "root_dir = os.path.dirname(os.path.abspath(''))\n",
    "root_dir\n",
    "sys.path.append(root_dir)\n",
    "from endpoints import MODEL_NAME_TO_ID,ask_frontier_llm,ask_nim_llm\n",
    "env_path=os.path.join(root_dir,'endpoints','.env')\n",
    "env_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf25e59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-01-preview\n",
      "{'clds35': 'claude-3-5-sonnet-20241022', 'clds37': 'claude-3-7-sonnet-20250219', 'clds4': 'claude-sonnet-4-20250514', 'cldo4': 'claude-opus-4-20250514', 'gpt-4o': 'gpt-4o-20241120', 'gpt-4o-mini': 'gpt-4o-mini-20240718', 'gpt-4-turbo': 'gpt-4-turbo-20240409', 'o1-preview': 'o1-preview-20240912', 'o1-mini': 'o1-mini-20240912', 'o1': 'o1-20241217', 'o3mini': 'o3-mini-20250131', 'llama3.3': 'nvdev/meta/llama-3.3-70b-instruct', 'dsr1': 'nvdev/deepseek-ai/deepseek-r1'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display_markdown\n",
    "# Remember to load the environment variables. You should have the Groq API Key in there :)\n",
    "load_dotenv(env_path)\n",
    "api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpt=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "perlab_api_key = os.getenv(\"PERFLAB_API_KEY\")\n",
    "\n",
    "print(api_version)\n",
    "print(MODEL_NAME_TO_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3dc761",
   "metadata": {},
   "source": [
    "### Quick sanity check and usage example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72eda3",
   "metadata": {},
   "source": [
    "#### Frontier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657fecf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a clean and well-documented implementation of the Merge Sort algorithm in Python:\n",
       "\n",
       "```python\n",
       "def merge_sort(arr):\n",
       "    \"\"\"\n",
       "    Sorts an array using the merge sort algorithm.\n",
       "    \n",
       "    Args:\n",
       "        arr (list): The input array to be sorted\n",
       "        \n",
       "    Returns:\n",
       "        list: The sorted array\n",
       "    \"\"\"\n",
       "    # Base case: if array has 1 or 0 elements, it's already sorted\n",
       "    if len(arr) <= 1:\n",
       "        return arr\n",
       "    \n",
       "    # Divide the array into two halves\n",
       "    mid = len(arr) // 2\n",
       "    left = arr[:mid]\n",
       "    right = arr[mid:]\n",
       "    \n",
       "    # Recursively sort both halves\n",
       "    left = merge_sort(left)\n",
       "    right = merge_sort(right)\n",
       "    \n",
       "    # Merge the sorted halves\n",
       "    return merge(left, right)\n",
       "\n",
       "def merge(left, right):\n",
       "    \"\"\"\n",
       "    Merges two sorted arrays into a single sorted array.\n",
       "    \n",
       "    Args:\n",
       "        left (list): First sorted array\n",
       "        right (list): Second sorted array\n",
       "        \n",
       "    Returns:\n",
       "        list: Merged sorted array\n",
       "    \"\"\"\n",
       "    result = []\n",
       "    i = j = 0\n",
       "    \n",
       "    # Compare elements from both arrays and merge them in sorted order\n",
       "    while i < len(left) and j < len(right):\n",
       "        if left[i] <= right[j]:\n",
       "            result.append(left[i])\n",
       "            i += 1\n",
       "        else:\n",
       "            result.append(right[j])\n",
       "            j += 1\n",
       "    \n",
       "    # Add remaining elements from left array, if any\n",
       "    result.extend(left[i:])\n",
       "    \n",
       "    # Add remaining elements from right array, if any\n",
       "    result.extend(right[j:])\n",
       "    \n",
       "    return result\n",
       "\n",
       "# Example usage\n",
       "if __name__ == \"__main__\":\n",
       "    # Test the merge sort implementation\n",
       "    test_array = [64, 34, 25, 12, 22, 11, 90]\n",
       "    print(\"Original array:\", test_array)\n",
       "    \n",
       "    sorted_array = merge_sort(test_array)\n",
       "    print(\"Sorted array:\", sorted_array)\n",
       "```\n",
       "\n",
       "This implementation includes:\n",
       "\n",
       "1. A main `merge_sort` function that implements the divide-and-conquer strategy:\n",
       "   - Divides the array into two halves\n",
       "   - Recursively sorts each half\n",
       "   - Merges the sorted halves\n",
       "\n",
       "2. A helper `merge` function that combines two sorted arrays into a single sorted array:\n",
       "   - Uses two pointers to track progress through each array\n",
       "   - Compares elements and builds the result array in sorted order\n",
       "   - Handles remaining elements from either array\n",
       "\n",
       "3. Clear documentation with docstrings explaining the purpose and parameters of each function\n",
       "\n",
       "4. Example usage that demonstrates how to use the sorting algorithm\n",
       "\n",
       "The algorithm has the following characteristics:\n",
       "- Time complexity: O(n log n)\n",
       "- Space complexity: O(n)\n",
       "- Stable sort: Yes (maintains relative order of equal elements)\n",
       "\n",
       "To use this implementation, you can simply call `merge_sort(your_array)` with any list of comparable elements. The function will return a new sorted list, leaving the original array unchanged.\n",
       "\n",
       "The example usage will output:\n",
       "```python\n",
       "Original array: [64, 34, 25, 12, 22, 11, 90]\n",
       "Sorted array: [11, 12, 22, 25, 34, 64, 90]\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for frontier models \n",
    "model_id=MODEL_NAME_TO_ID['clds35']\n",
    "client = AzureOpenAI(azure_endpoint=azure_endpt,\n",
    "                     api_version=api_version,\n",
    "                     api_key=api_key)\n",
    "\n",
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "\n",
    "mergesort_code = client.chat.completions.create(\n",
    "    messages=generation_chat_history,\n",
    "    model=model_id\n",
    ").choices[0].message.content\n",
    "\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": mergesort_code\n",
    "    }\n",
    ")\n",
    "display_markdown(mergesort_code, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=MODEL_NAME_TO_ID['clds35']\n",
    "user_prompt = \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "system_prompt = \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "mergesort_code = ask_frontier_llm(system_prompt,user_prompt,model_id)\n",
    "display_markdown(mergesort_code,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299ae08",
   "metadata": {},
   "source": [
    "## Accessing through Langchain \n",
    "\n",
    "Make sure you have the required libraries installed : \n",
    "```bash\n",
    "! pip install langchain langgraph langchain_openai \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a4ab1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I'm working and ready to help! What can I assist you with today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpt,\n",
    "    api_version=api_version,\n",
    "    api_key=api_key,\n",
    "    model=MODEL_NAME_TO_ID['clds35'],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Test the setup\n",
    "response = llm.invoke(\"Hello! Are you working?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e59fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a clean and well-documented implementation of the Merge Sort algorithm in Python:\n",
      "\n",
      "\n",
      "def merge_sort(arr):\n",
      "    \"\"\"\n",
      "    Sorts an array using the merge sort algorithm.\n",
      "    \n",
      "    Args:\n",
      "        arr: List of comparable elements\n",
      "        \n",
      "    Returns:\n",
      "        Sorted list in ascending order\n",
      "    \"\"\"\n",
      "    # Base case: if array has 1 or fewer elements, it's already sorted\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    \n",
      "    # Divide the array into two halves\n",
      "    mid = len(arr) // 2\n",
      "    left = arr[:mid]\n",
      "    right = arr[mid:]\n",
      "    \n",
      "    # Recursively sort both halves\n",
      "    left = merge_sort(left)\n",
      "    right = merge_sort(right)\n",
      "    \n",
      "    # Merge the sorted halves\n",
      "    return merge(left, right)\n",
      "\n",
      "def merge(left, right):\n",
      "    \"\"\"\n",
      "    Merges two sorted arrays into a single sorted array.\n",
      "    \n",
      "    Args:\n",
      "        left: First sorted array\n",
      "        right: Second sorted array\n",
      "        \n",
      "    Returns:\n",
      "        Merged sorted array\n",
      "    \"\"\"\n",
      "    result = []\n",
      "    left_idx = right_idx = 0\n",
      "    \n",
      "    # Compare elements from both arrays and merge them in sorted order\n",
      "    while left_idx < len(left) and right_idx < len(right):\n",
      "        if left[left_idx] <= right[right_idx]:\n",
      "            result.append(left[left_idx])\n",
      "            left_idx += 1\n",
      "        else:\n",
      "            result.append(right[right_idx])\n",
      "            right_idx += 1\n",
      "    \n",
      "    # Add remaining elements from left array, if any\n",
      "    result.extend(left[left_idx:])\n",
      "    \n",
      "    # Add remaining elements from right array, if any\n",
      "    result.extend(right[right_idx:])\n",
      "    \n",
      "    return result\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    # Test the merge sort implementation\n",
      "    test_arrays = [\n",
      "        [64, 34, 25, 12, 22, 11, 90],\n",
      "        [5, 2, 8, 1, 9],\n",
      "        [1],\n",
      "        [],\n",
      "        [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]\n",
      "    ]\n",
      "    \n",
      "    for arr in test_arrays:\n",
      "        print(f\"Original array: {arr}\")\n",
      "        sorted_arr = merge_sort(arr)\n",
      "        print(f\"Sorted array: {sorted_arr}\")\n",
      "        print()\n",
      "\n",
      "\n",
      "This implementation includes:\n",
      "\n",
      "1. The main `merge_sort` function that implements the recursive divide-and-conquer strategy:\n",
      "   - Divides the array into two halves\n",
      "   - Recursively sorts each half\n",
      "   - Merges the sorted halves\n",
      "\n",
      "2. A helper `merge` function that combines two sorted arrays into a single sorted array:\n",
      "   - Uses two pointers to track progress through each array\n",
      "   - Compares elements and builds the merged result\n",
      "   - Handles remaining elements from either array\n",
      "\n",
      "3. Example usage with test cases demonstrating the algorithm's functionality with:\n",
      "   - Regular arrays\n",
      "   - Small arrays\n",
      "   - Single-element arrays\n",
      "   - Empty arrays\n",
      "\n",
      "Key features of this implementation:\n",
      "\n",
      "- Time Complexity: O(n log n) in all cases\n",
      "- Space Complexity: O(n)\n",
      "- Stable sorting algorithm (maintains relative order of equal elements)\n",
      "- Clear documentation and comments\n",
      "- Clean, readable code structure\n",
      "\n",
      "The algorithm works by:\n",
      "1. Dividing the input array into smaller subarrays until each subarray contains one element\n",
      "2. Merging these subarrays back together in sorted order\n",
      "3. Continuing this process until the entire array is sorted\n",
      "\n",
      "When you run this code, it will demonstrate the sorting process with various test cases. You can use it like this:\n",
      "\n",
      "\n",
      "arr = [64, 34, 25, 12, 22, 11, 90]\n",
      "sorted_arr = merge_sort(arr)\n",
      "print(sorted_arr)  # Output: [11, 12, 22, 25, 34, 64, 90]\n"
     ]
    }
   ],
   "source": [
    "model_id=MODEL_NAME_TO_ID['clds35']\n",
    "user_prompt_template = \"\"\"Generate a Python implementation of the {algo_name} algorithm\"\"\"\n",
    "system_prompt_template = \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_template),\n",
    "    (\"human\", user_prompt_template)\n",
    "])\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpt,\n",
    "    api_version=api_version,\n",
    "    api_key=api_key,\n",
    "    model=model_id,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "chain = template | llm\n",
    "\n",
    "response = chain.invoke({\"algo_name\": \"Merge Sort\"})\n",
    "\n",
    "# Clean up the response, removing markdown code fences\n",
    "clean_code = response.content.strip().replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "print(clean_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d0909",
   "metadata": {},
   "source": [
    "#### NIM models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=MODEL_NAME_TO_ID['llama3.3']\n",
    "user_prompt = \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "system_prompt = \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "mergesort_code = ask_nim_llm(system_prompt,user_prompt,model_id)\n",
    "display_markdown(mergesort_code,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080eb0a0",
   "metadata": {},
   "source": [
    "# Accessing local (HF) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-32B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-32B\",torch_dtype=torch.bfloat16,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "generation_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way to generate the prompt\n",
    "prompt = \"\"\n",
    "for message in generation_chat_history:\n",
    "    if message[\"role\"] == \"system\":\n",
    "        prompt += f\"System: {message['content']}\\n\"\n",
    "    elif message[\"role\"] == \"user\":\n",
    "        prompt += f\"User: {message['content']}\\n\"\n",
    "    elif message[\"role\"] == \"assistant\":\n",
    "        prompt += f\"Assistant: {message['content']}\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "prompt_length = inputs[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2bc33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more direct\n",
    "inputs2 = tokenizer.apply_chat_template(\n",
    "    generation_chat_history,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "prompt_length = inputs2.shape[1]  # Number of tokens in the prompt\n",
    "# inputs = {k: v.to(model.device) for k, v in inputs2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd5b1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs2,\n",
    "        max_new_tokens=4096,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc57601",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = output[0][prompt_length:]\n",
    "\n",
    "# 5. Decode only the new tokens\n",
    "assistant_response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b9480",
   "metadata": {},
   "source": [
    "## Testing the generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bb35e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    \"\"\"\n",
    "    Sorts a list using the Merge Sort algorithm.\n",
    "    \n",
    "    Args:\n",
    "        arr (list): The list to be sorted.\n",
    "        \n",
    "    Returns:\n",
    "        list: A new sorted list.\n",
    "    \"\"\"\n",
    "    # Base case: if the array has one element or is empty, it's already sorted\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    \n",
    "    # Divide the array into two halves\n",
    "    mid = len(arr) // 2\n",
    "    left_half = merge_sort(arr[:mid])  # Recursively sort the left half\n",
    "    right_half = merge_sort(arr[mid:])  # Recursively sort the right half\n",
    "    \n",
    "    # Combine the sorted halves\n",
    "    return merge(left_half, right_half)\n",
    "\n",
    "\n",
    "def merge(left, right):\n",
    "    \"\"\"\n",
    "    Merges two sorted lists into a single sorted list.\n",
    "    \n",
    "    Args:\n",
    "        left (list): The first sorted list.\n",
    "        right (list): The second sorted list.\n",
    "        \n",
    "    Returns:\n",
    "        list: A merged sorted list.\n",
    "    \"\"\"\n",
    "    merged = []  # Result list\n",
    "    i = j = 0    # Pointers for left and right lists\n",
    "    \n",
    "    # Merge the two lists by comparing elements\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] <= right[j]:  # Ensure stability by using <=\n",
    "            merged.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            j += 1\n",
    "    \n",
    "    # Add any remaining elements from left and right\n",
    "    merged.extend(left[i:])\n",
    "    merged.extend(right[j:])\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsorted = [34, 7, 23, 32, 5, 62]\n",
    "unsorted = [64, 34, 25, 12, 22, 11, 90]\n",
    "sorted_list = merge_sort(unsorted)\n",
    "print(sorted_list)  # Output: [5, 7, 23, 32, 34, 62]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e979c5",
   "metadata": {},
   "source": [
    "# Accessing inference service\n",
    "using ollama :\n",
    "- setup docker network : \n",
    "```bash\n",
    "docker network create llmnet\n",
    "```\n",
    "- launch an ollama container \n",
    "```bash\n",
    "docker run -d  --gpus all --name ollama   --network llmnet   -p 11434:11434  ollama/ollama\n",
    "```\n",
    "\n",
    "Note: make sure that this notebook's container is also launched with `--network llmnet`\n",
    "\n",
    "- attach to the ollama container to pull the model\n",
    "```bash\n",
    "docker exec -it ollama bash\n",
    "```\n",
    "- from within the container, pull the model\n",
    "```\n",
    "ollama pull qwen3:8b\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b23a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d7461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(messages):\n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            prompt += f\"System: {msg['content']}\\n\"\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            prompt += f\"User: {msg['content']}\\n\"\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {msg['content']}\\n\"\n",
    "    response = requests.post(\n",
    "        \"http://ollama:11434/api/generate\",\n",
    "        json={\"model\": \"qwen3:8b\", \"prompt\": prompt}\n",
    "    )\n",
    "    print(response.status_code)\n",
    "    full_text = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode('utf-8'))\n",
    "            # The generated text is usually in the 'response' field\n",
    "            full_text += data.get(\"response\", \"\")\n",
    "    return full_text\n",
    "\n",
    "def get_response_openai(messages, model=\"qwen3:8b\", base_url=\"http://ollama:11434/v1\"):\n",
    "    # Create a client that points to the Ollama OpenAI-compatible endpoint\n",
    "    client = openai.OpenAI(\n",
    "        api_key=\"ollama\",  # Any string, Ollama doesn't check it\n",
    "        base_url=base_url\n",
    "    )\n",
    "    # Call the chat completion endpoint\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    # Extract the assistant's reply\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aceead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "generation_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using native ollama api\n",
    "response = get_response(generation_chat_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using openai api\n",
    "response = get_response_openai(generation_chat_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f6e48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    \"\"\"\n",
    "    Sorts an array using the Merge Sort algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    arr (list): The list of elements to be sorted.\n",
    "    \n",
    "    Returns:\n",
    "    list: A new list containing all elements from the original list, sorted in ascending order.\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr  # Base case: single-element list is already sorted\n",
    "    \n",
    "    # Split the array into left and right halves\n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort(arr[:mid])  # Recursively sort the left half\n",
    "    right = merge_sort(arr[mid:])  # Recursively sort the right half\n",
    "    \n",
    "    # Merge the sorted halves\n",
    "    return merge(left, right)\n",
    "\n",
    "def merge(left, right):\n",
    "    \"\"\"\n",
    "    Merges two sorted lists into a single sorted list.\n",
    "    \n",
    "    Parameters:\n",
    "    left (list): The first sorted list.\n",
    "    right (list): The second sorted list.\n",
    "    \n",
    "    Returns:\n",
    "    list: A new list containing all elements from both input lists, sorted in ascending order.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = j = 0\n",
    "    \n",
    "    # Merge elements from both lists\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] < right[j]:\n",
    "            merged.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            j += 1\n",
    "    \n",
    "    # Add any remaining elements from the left or right list\n",
    "    merged.extend(left[i:])\n",
    "    merged.extend(right[j:])\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsorted = [34, 7, 23, 32, 5, 62]\n",
    "unsorted = [64, 34, 25, 12, 22, 11, 90]\n",
    "sorted_list = merge_sort(unsorted)\n",
    "print(sorted_list)  # Output: [5, 7, 23, 32, 34, 62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46fbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
