{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d86f85d",
   "metadata": {},
   "source": [
    "# Endpoint Access\n",
    "The goal of this notebook is to examine the ability of a kernel agent to generate kernels that are memory bw limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd273f",
   "metadata": {},
   "source": [
    "## Setup and Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22527694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gkoren/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/gkoren/code/github/guyknvda/openevolve/gk_sandbox/endpoints/.env'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "root_dir = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
    "root_dir\n",
    "sys.path.append(root_dir)\n",
    "from gk_sandbox.endpoints.endpoints import MODEL_NAME_TO_ID,ask_frontier_llm,ask_nim_llm\n",
    "env_path=os.path.join(root_dir,'gk_sandbox','endpoints','.env')\n",
    "env_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display_markdown\n",
    "# Remember to load the environment variables. You should have the Groq API Key in there :)\n",
    "load_dotenv(env_path)\n",
    "api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpt=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "perlab_api_key = os.getenv(\"PERFLAB_API_KEY\")\n",
    "\n",
    "print(api_version)\n",
    "print(MODEL_NAME_TO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb87e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_endpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3dc761",
   "metadata": {},
   "source": [
    "### Quick sanity check and usage example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226174b",
   "metadata": {},
   "source": [
    "#### Frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for frontier models \n",
    "model_id=MODEL_NAME_TO_ID['clds35']\n",
    "client = AzureOpenAI(azure_endpoint=azure_endpt,\n",
    "                     api_version=api_version,\n",
    "                     api_key=api_key)\n",
    "\n",
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "\n",
    "mergesort_code = client.chat.completions.create(\n",
    "    messages=generation_chat_history,\n",
    "    model=model_id\n",
    ").choices[0].message.content\n",
    "\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": mergesort_code\n",
    "    }\n",
    ")\n",
    "display_markdown(mergesort_code, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fc4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=MODEL_NAME_TO_ID['clds35']\n",
    "user_prompt = \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "system_prompt = \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "mergesort_code = ask_frontier_llm(system_prompt,user_prompt,model_id)\n",
    "display_markdown(mergesort_code,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee4ffc",
   "metadata": {},
   "source": [
    "#### NIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6091624",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=MODEL_NAME_TO_ID['llama3.3']\n",
    "user_prompt = \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "system_prompt = \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "mergesort_code = ask_nim_llm(system_prompt,user_prompt,model_id)\n",
    "display_markdown(mergesort_code,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080eb0a0",
   "metadata": {},
   "source": [
    "# Accessing local (HF) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-32B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-32B\",torch_dtype=torch.bfloat16,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "generation_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way to generate the prompt\n",
    "prompt = \"\"\n",
    "for message in generation_chat_history:\n",
    "    if message[\"role\"] == \"system\":\n",
    "        prompt += f\"System: {message['content']}\\n\"\n",
    "    elif message[\"role\"] == \"user\":\n",
    "        prompt += f\"User: {message['content']}\\n\"\n",
    "    elif message[\"role\"] == \"assistant\":\n",
    "        prompt += f\"Assistant: {message['content']}\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "prompt_length = inputs[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2bc33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more direct\n",
    "inputs2 = tokenizer.apply_chat_template(\n",
    "    generation_chat_history,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "prompt_length = inputs2.shape[1]  # Number of tokens in the prompt\n",
    "# inputs = {k: v.to(model.device) for k, v in inputs2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd5b1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs2,\n",
    "        max_new_tokens=4096,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc57601",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = output[0][prompt_length:]\n",
    "\n",
    "# 5. Decode only the new tokens\n",
    "assistant_response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b9480",
   "metadata": {},
   "source": [
    "## Testing the generation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bb35e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    \"\"\"\n",
    "    Sorts a list using the Merge Sort algorithm.\n",
    "    \n",
    "    Args:\n",
    "        arr (list): The list to be sorted.\n",
    "        \n",
    "    Returns:\n",
    "        list: A new sorted list.\n",
    "    \"\"\"\n",
    "    # Base case: if the array has one element or is empty, it's already sorted\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    \n",
    "    # Divide the array into two halves\n",
    "    mid = len(arr) // 2\n",
    "    left_half = merge_sort(arr[:mid])  # Recursively sort the left half\n",
    "    right_half = merge_sort(arr[mid:])  # Recursively sort the right half\n",
    "    \n",
    "    # Combine the sorted halves\n",
    "    return merge(left_half, right_half)\n",
    "\n",
    "\n",
    "def merge(left, right):\n",
    "    \"\"\"\n",
    "    Merges two sorted lists into a single sorted list.\n",
    "    \n",
    "    Args:\n",
    "        left (list): The first sorted list.\n",
    "        right (list): The second sorted list.\n",
    "        \n",
    "    Returns:\n",
    "        list: A merged sorted list.\n",
    "    \"\"\"\n",
    "    merged = []  # Result list\n",
    "    i = j = 0    # Pointers for left and right lists\n",
    "    \n",
    "    # Merge the two lists by comparing elements\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] <= right[j]:  # Ensure stability by using <=\n",
    "            merged.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            j += 1\n",
    "    \n",
    "    # Add any remaining elements from left and right\n",
    "    merged.extend(left[i:])\n",
    "    merged.extend(right[j:])\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsorted = [34, 7, 23, 32, 5, 62]\n",
    "unsorted = [64, 34, 25, 12, 22, 11, 90]\n",
    "sorted_list = merge_sort(unsorted)\n",
    "print(sorted_list)  # Output: [5, 7, 23, 32, 34, 62]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e979c5",
   "metadata": {},
   "source": [
    "# Accessing inference service\n",
    "using ollama :\n",
    "- setup docker network : \n",
    "```bash\n",
    "docker network create llmnet\n",
    "```\n",
    "- launch an ollama container \n",
    "```bash\n",
    "docker run -d  --gpus all --name ollama   --network llmnet   -p 11434:11434  ollama/ollama\n",
    "```\n",
    "\n",
    "Note: make sure that this notebook's container is also launched with `--network llmnet`\n",
    "\n",
    "- attach to the ollama container to pull the model\n",
    "```bash\n",
    "docker exec -it ollama bash\n",
    "```\n",
    "- from within the container, pull the model\n",
    "```\n",
    "ollama pull qwen3:8b\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b23a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d7461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(messages):\n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            prompt += f\"System: {msg['content']}\\n\"\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            prompt += f\"User: {msg['content']}\\n\"\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {msg['content']}\\n\"\n",
    "    response = requests.post(\n",
    "        \"http://ollama:11434/api/generate\",\n",
    "        json={\"model\": \"qwen3:8b\", \"prompt\": prompt}\n",
    "    )\n",
    "    print(response.status_code)\n",
    "    full_text = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode('utf-8'))\n",
    "            # The generated text is usually in the 'response' field\n",
    "            full_text += data.get(\"response\", \"\")\n",
    "    return full_text\n",
    "\n",
    "def get_response_openai(messages, model=\"qwen3:8b\", base_url=\"http://ollama:11434/v1\"):\n",
    "    # Create a client that points to the Ollama OpenAI-compatible endpoint\n",
    "    client = openai.OpenAI(\n",
    "        api_key=\"ollama\",  # Any string, Ollama doesn't check it\n",
    "        base_url=base_url\n",
    "    )\n",
    "    # Call the chat completion endpoint\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    # Extract the assistant's reply\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aceead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")\n",
    "generation_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using native ollama api\n",
    "response = get_response(generation_chat_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using openai api\n",
    "response = get_response_openai(generation_chat_history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f6e48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr):\n",
    "    \"\"\"\n",
    "    Sorts an array using the Merge Sort algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    arr (list): The list of elements to be sorted.\n",
    "    \n",
    "    Returns:\n",
    "    list: A new list containing all elements from the original list, sorted in ascending order.\n",
    "    \"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr  # Base case: single-element list is already sorted\n",
    "    \n",
    "    # Split the array into left and right halves\n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort(arr[:mid])  # Recursively sort the left half\n",
    "    right = merge_sort(arr[mid:])  # Recursively sort the right half\n",
    "    \n",
    "    # Merge the sorted halves\n",
    "    return merge(left, right)\n",
    "\n",
    "def merge(left, right):\n",
    "    \"\"\"\n",
    "    Merges two sorted lists into a single sorted list.\n",
    "    \n",
    "    Parameters:\n",
    "    left (list): The first sorted list.\n",
    "    right (list): The second sorted list.\n",
    "    \n",
    "    Returns:\n",
    "    list: A new list containing all elements from both input lists, sorted in ascending order.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = j = 0\n",
    "    \n",
    "    # Merge elements from both lists\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] < right[j]:\n",
    "            merged.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            merged.append(right[j])\n",
    "            j += 1\n",
    "    \n",
    "    # Add any remaining elements from the left or right list\n",
    "    merged.extend(left[i:])\n",
    "    merged.extend(right[j:])\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsorted = [34, 7, 23, 32, 5, 62]\n",
    "unsorted = [64, 34, 25, 12, 22, 11, 90]\n",
    "sorted_list = merge_sort(unsorted)\n",
    "print(sorted_list)  # Output: [5, 7, 23, 32, 34, 62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46fbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
